x-default: &default
  restart: unless-stopped
  logging:
    driver: json-file
    options:
      max-size: 100m
      max-file: "3"

services:
  ollama:
    <<: *default
    image: ollama/ollama:${OLLAMA_VERSION:-0.12.6}
    ports:
      - "${OLLAMA_PORT_OVERRIDE:-11434}:11434"
    volumes:
      - ollama_models:/root/.ollama
    environment:
      - TZ=${TZ:-UTC}
    ipc: host
    deploy:
      resources:
        limits:
          cpus: '8.0'
          memory: 4G
        reservations:
          cpus: '2.0'
          memory: 2G
          devices:
            - driver: nvidia
              device_ids: [ '0' ]
              capabilities: [ gpu ]
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:11434/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

volumes:
  ollama_models:

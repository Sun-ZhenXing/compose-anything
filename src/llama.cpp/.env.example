# =============================================================================
# llama.cpp Configuration
# https://github.com/ggml-org/llama.cpp
# LLM inference in C/C++ with support for various hardware accelerators
# =============================================================================

# -----------------------------------------------------------------------------
# General Settings
# -----------------------------------------------------------------------------

# Timezone for the container (default: UTC)
TZ=UTC

# Global registry prefix (optional)
# Example: docker.io/, ghcr.io/, registry.example.com/
GHCR_REGISTRY=ghcr.io/

# -----------------------------------------------------------------------------
# Server Configuration
# -----------------------------------------------------------------------------

# Server image variant
# Options: server (CPU), server-cuda (NVIDIA GPU), server-rocm (AMD GPU),
#          server-musa (Moore Threads GPU), server-intel (Intel GPU),
#          server-vulkan (Vulkan GPU)
LLAMA_CPP_SERVER_VARIANT=server

# Server port override (default: 8080)
LLAMA_CPP_SERVER_PORT_OVERRIDE=8080

# Model path inside the container
# You need to mount your model file to this path
# Example: /models/llama-2-7b-chat.Q4_K_M.gguf
LLAMA_CPP_MODEL_PATH=/models/model.gguf

# Context size (number of tokens)
# Larger values allow for more context but require more memory
# Default: 512, Common values: 512, 2048, 4096, 8192, 16384, 32768
LLAMA_CPP_CONTEXT_SIZE=512

# Number of GPU layers to offload
# 0 = CPU only, 99 = all layers on GPU (for GPU variants)
# For CPU variant, keep this at 0
LLAMA_CPP_GPU_LAYERS=0

# Number of GPUs to use (for CUDA variant)
LLAMA_CPP_GPU_COUNT=1

# Server CPU limit (in cores)
LLAMA_CPP_SERVER_CPU_LIMIT=4.0

# Server CPU reservation (in cores)
LLAMA_CPP_SERVER_CPU_RESERVATION=2.0

# Server memory limit
LLAMA_CPP_SERVER_MEMORY_LIMIT=8G

# Server memory reservation
LLAMA_CPP_SERVER_MEMORY_RESERVATION=4G

# -----------------------------------------------------------------------------
# CLI Configuration (Light variant)
# -----------------------------------------------------------------------------

# CLI image variant
# Options: light (CPU), light-cuda (NVIDIA GPU), light-rocm (AMD GPU),
#          light-musa (Moore Threads GPU), light-intel (Intel GPU),
#          light-vulkan (Vulkan GPU)
LLAMA_CPP_CLI_VARIANT=light

# Default prompt for CLI mode
LLAMA_CPP_PROMPT=Hello, how are you?

# CLI CPU limit (in cores)
LLAMA_CPP_CLI_CPU_LIMIT=2.0

# CLI CPU reservation (in cores)
LLAMA_CPP_CLI_CPU_RESERVATION=1.0

# CLI memory limit
LLAMA_CPP_CLI_MEMORY_LIMIT=4G

# CLI memory reservation
LLAMA_CPP_CLI_MEMORY_RESERVATION=2G

# -----------------------------------------------------------------------------
# Full Toolkit Configuration
# -----------------------------------------------------------------------------

# Full image variant (includes model conversion tools)
# Options: full (CPU), full-cuda (NVIDIA GPU), full-rocm (AMD GPU),
#          full-musa (Moore Threads GPU), full-intel (Intel GPU),
#          full-vulkan (Vulkan GPU)
LLAMA_CPP_FULL_VARIANT=full

# Full CPU limit (in cores)
LLAMA_CPP_FULL_CPU_LIMIT=2.0

# Full CPU reservation (in cores)
LLAMA_CPP_FULL_CPU_RESERVATION=1.0

# Full memory limit
LLAMA_CPP_FULL_MEMORY_LIMIT=4G

# Full memory reservation
LLAMA_CPP_FULL_MEMORY_RESERVATION=2G
